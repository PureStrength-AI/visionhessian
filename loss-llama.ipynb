{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4a24a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch                       # PyTorch library for deep learning\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,          # AutoModel for language modeling tasks\n",
    "    AutoTokenizer,                # AutoTokenizer for tokenization\n",
    "    BitsAndBytesConfig,           # Configuration for BitsAndBytes\n",
    "    HfArgumentParser,             # Argument parser for Hugging Face models\n",
    "    TrainingArguments,            # Training arguments for model training\n",
    "    pipeline,                     # Creating pipelines for model inference\n",
    "    logging,                      # Logging information during training\n",
    ")\n",
    "from peft import LoraConfig, PeftModel  # Packages for parameter-efficient fine-tuning (PEFT)\n",
    "from trl import SFTTrainer         # SFTTrainer for supervised fine-tuning \n",
    "import argparse\n",
    "from datasets import load_from_disk, Dataset, load_dataset\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "access_token = \"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\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "qlora_training = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_auth_token=access_token) # previous authentication with HuggingFace needed, see here https://huggingface.co/docs/huggingface_hub/main/en/package_reference/login\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "device_map = \"auto\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    use_auth_token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
    "\n",
    "# Calculate the size of the subsample (1% of the 'train' split)\n",
    "subsample_size = int(0.1 * len(ds['train']))\n",
    "\n",
    "# Create a random subsample of the dataset\n",
    "subsample = ds['train'].shuffle(seed=42).select(range(subsample_size))\n",
    "\n",
    "# Load tokenizer for your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# Tokenize the subsample\n",
    "def tokenize_function(examples):\n",
    "    # Truncation and padding are typically handled here if necessary\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_docs = subsample.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenized_docs[0]\n",
    "print(example.keys())  # Should include 'input_ids' and potentially 'attention_mask'\n",
    "print(example['input_ids'][:10])  # Print first 10 token IDs to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def select_model_inputs(batch):\n",
    "    return {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# Apply the function to filter out only the necessary fields\n",
    "model_inputs = tokenized_docs.map(select_model_inputs, batched=True)\n",
    "\n",
    "# Manually collate a batch\n",
    "def manual_collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# Create a DataLoader with the manual collate function\n",
    "dataloader = DataLoader(model_inputs, batch_size=16, collate_fn=manual_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f542b4",
   "metadata": {},
   "source": [
    "For each batch, the model performs a forward pass, receiving a batch of tokenized input sequences (input_ids) and the same sequences as labels (labels). In the context of causal language modeling (assuming the use of a model like GPT or similar from the Transformers library), the model attempts to predict each token in the sequence given the tokens that precede it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea152a4a",
   "metadata": {},
   "source": [
    "Loss: The model's output includes logits, which are the raw, unnormalized scores for each token in the vocabulary, for each position in the input sequence. When labels are provided (as they are here, with labels=batch[\"input_ids\"]), the model also calculates the loss. This loss is typically the Cross-Entropy Loss between the logits (predictions) and the provided labels (targets), averaged over the batch. In simpler terms, the loss measures how well the model's predictions match the actual next tokens in the input sequences.\n",
    "\n",
    "    The Cross-Entropy Loss is calculated for each token position in each sequence, comparing the model's prediction (the probability distribution over all possible tokens) with the actual token (represented as a one-hot encoded vector). The loss is high if the model's predicted probabilities diverge significantly from the actual token and low if the model's predictions are accurate.\n",
    "    Since the labels are the same as the input IDs (the model is predicting the next token in the sequence, and each token is used as its own label), the loss effectively measures how well the model can continue the sequence at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "losses = np.array([])  # A list to store our calculated losses\n",
    "c = 0\n",
    "# Assuming 'dataloader' is your DataLoader instance\n",
    "for batch in dataloader:\n",
    "    try:\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "            logits = outputs.logits  # The model's predictions\n",
    "            \n",
    "            # Calculate Cross Entropy Loss manually for each token.\n",
    "            # The model's logits are typically of shape [batch_size, seq_length, vocab_size],\n",
    "            # and you want to calculate the loss with respect to the true labels for each token.\n",
    "            # 'labels=batch[\"input_ids\"]' is used for simplicity; replace it as needed.\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), batch[\"input_ids\"].view(-1), reduction='none')\n",
    "            \n",
    "            # Reshape loss back to [batch_size, seq_length] to get per-token loss\n",
    "            per_token_loss = loss.view_as(batch[\"input_ids\"])\n",
    "            \n",
    "            # Here, you can process the per_token_loss as needed, for example, averaging over tokens.\n",
    "            # This example just stores them.\n",
    "#             losses.append(per_token_loss.cpu().numpy())\n",
    "            losses = np.concatenate((losses,per_token_loss.cpu().numpy().flatten()))\n",
    "            if c % 2 == 0:\n",
    "                with open('losses_pertoken.pkl', 'wb') as file:\n",
    "                    # Use pickle.dump() to write the list to the file\n",
    "                    pickle.dump(losses, file)\n",
    "                    print('{}% of iterations done'.format(c/len(dataloader)))\n",
    "            c+=1\n",
    "            \n",
    "            print('Batch processed')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "# Further processing on `losses` as needed...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a848a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "losses = [] \n",
    "k = 0\n",
    "for batch in dataloader:\n",
    "    try:\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "            losses.append(outputs.loss.item())\n",
    "            if k % 10 == 0:\n",
    "                print(f\"Batch loss: {outputs.loss.item()}\")\n",
    "                with open('losses.pkl', 'wb') as file:\n",
    "                    # Use pickle.dump() to write the list to the file\n",
    "                    pickle.dump(losses, file)\n",
    "            k+=1\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d81f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
