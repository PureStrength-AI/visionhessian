{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c066eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "def layer_by_layer_hess_vec(dataloader, model, cuda=True, bn_train_mode=False):\n",
    "    # Prepare the model\n",
    "    model.eval()\n",
    "    if bn_train_mode:\n",
    "        # Custom function to set batch normalization layers to train mode\n",
    "        model.apply(_bn_train_mode)\n",
    "    \n",
    "    # Initialize a dictionary to store Hessian-vector products for each layer\n",
    "    layerwise_hvp = {}\n",
    "\n",
    "    # Iterate through each parameter layer in the model\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.requires_grad:\n",
    "            # Initialize a vector of ones with the same shape as the parameter\n",
    "            vec = torch.ones_like(parameter)\n",
    "            \n",
    "            # Ensure vector is on the same device as the model\n",
    "            if cuda:\n",
    "                vec = vec.cuda()\n",
    "            \n",
    "            # Zero gradients in the model\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Compute the loss for the given data loader\n",
    "            total_loss = 0\n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                input_ids = batch[\"input_ids\"].to(\"cuda\" if cuda else \"cpu\")\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:  # Check if loss is not scalar\n",
    "                    loss = loss.mean()\n",
    "                total_loss += loss\n",
    "            \n",
    "            # Normalize the loss\n",
    "            total_loss /= len(dataloader)\n",
    "            \n",
    "            # Compute gradients with respect to the target parameter\n",
    "            grad_loss = grad(total_loss, parameter, create_graph=True)[0]\n",
    "            \n",
    "            # Compute the Hessian-vector product for the current parameter\n",
    "            hvp = grad(grad_loss, parameter, grad_outputs=vec)[0]\n",
    "            \n",
    "            # Store the computed Hessian-vector product\n",
    "            layerwise_hvp[name] = hvp.detach()  # Detach to avoid saving in the computation graph\n",
    "    \n",
    "    return layerwise_hvp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ef6ce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Compute the layer-wise Hessian\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m layer_hessians \u001b[38;5;241m=\u001b[39m compute_layerwise_hessian(model, inputs, labels, criterion)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Example: print the shape of the Hessian for the first layer\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(layer_hessians[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m, in \u001b[0;36mcompute_layerwise_hessian\u001b[0;34m(model, inputs, labels, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, parameter \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parameter\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Compute the gradient of the loss with respect to the current parameter\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         grad_loss \u001b[38;5;241m=\u001b[39m grad(loss, parameter, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Initialize a vector for Hessian computation that matches the parameter shape\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(parameter)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:394\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    390\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    391\u001b[0m         grad_outputs_\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    395\u001b[0m         t_outputs,\n\u001b[1;32m    396\u001b[0m         grad_outputs_,\n\u001b[1;32m    397\u001b[0m         retain_graph,\n\u001b[1;32m    398\u001b[0m         create_graph,\n\u001b[1;32m    399\u001b[0m         t_inputs,\n\u001b[1;32m    400\u001b[0m         allow_unused,\n\u001b[1;32m    401\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    402\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    405\u001b[0m         output\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (output, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, t_inputs)\n\u001b[1;32m    409\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Example model (for demonstration)\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load a sample dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the model and criterion\n",
    "model = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get a single batch from the DataLoader\n",
    "inputs, labels = next(iter(train_loader))\n",
    "\n",
    "# Compute the layer-wise Hessian\n",
    "layer_hessians = compute_layerwise_hessian(model, inputs, labels, criterion)\n",
    "\n",
    "# Example: print the shape of the Hessian for the first layer\n",
    "print(layer_hessians['fc1.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110a533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
