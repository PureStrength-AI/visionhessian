{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7915de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a620fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from torch.optim import SGD\n",
    "import torch \n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
    "\n",
    "# Calculate the size of the subsample (1% of the 'train' split)\n",
    "subsample_size = int(0.1 * len(ds['train']))\n",
    "\n",
    "# Create a random subsample of the dataset\n",
    "subsample = ds['train'].shuffle(seed=42).select(range(subsample_size))\n",
    "\n",
    "# Load tokenizer for your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# Tokenize the subsample\n",
    "def tokenize_function(examples):\n",
    "    # Truncation and padding are typically handled here if necessary\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_docs = subsample.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a5c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'url', 'title', 'text', 'input_ids', 'attention_mask'])\n",
      "[36949, 16698, 46082, 1081, 354, 357, 17543, 1478, 11, 41435]\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_docs[0]\n",
    "print(example.keys())  # Should include 'input_ids' and potentially 'attention_mask'\n",
    "print(example['input_ids'][:10])  # Print first 10 token IDs to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420b48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def select_model_inputs(batch):\n",
    "    return {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# Apply the function to filter out only the necessary fields\n",
    "model_inputs = tokenized_docs.map(select_model_inputs, batched=True)\n",
    "\n",
    "# Manually collate a batch\n",
    "def manual_collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "dataloader = DataLoader(model_inputs, batch_size=16, collate_fn=manual_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734d3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(vocab_size=len(tokenizer), n_positions=512)\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.to(torch.device(\"cuda\"))\n",
    "optimizer = SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e65ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.225703239440918\n",
      "Loss: 6.9129319190979\n",
      "Loss: 4.700535297393799\n",
      "Loss: 4.12257719039917\n",
      "Loss: 4.641554355621338\n",
      "Loss: 4.440639972686768\n",
      "Loss: 3.6101434230804443\n",
      "Loss: 1.9917831420898438\n",
      "Loss: 3.634214162826538\n",
      "Loss: 3.321385383605957\n",
      "Loss: 3.134232997894287\n",
      "Loss: 4.1511406898498535\n",
      "Loss: 3.767796277999878\n",
      "Loss: 3.223653793334961\n",
      "Loss: 2.725511312484741\n",
      "Loss: 4.277787208557129\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
