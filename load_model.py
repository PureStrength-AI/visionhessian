# -*- coding: utf-8 -*-
"""VGG CIFAR Hessian updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1thL21_zjDfPG3MpCY2kdI0CmKErgWNHH
"""

import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import argparse
# Create a normalized random vector
import time
import gpytorch
import os
import numpy as np

parser = argparse.ArgumentParser(description='Example script to demonstrate argparse usage.')
# Positional argument

# Optional argument
# parser.add_argument('--output', type=str, help='Output file path')
# parser.add_argument('--model', type=str, help='Output file path', default="vgg")
parser.add_argument('--ckpt', type=str, help='Output file path', default="vgg")
# Flag (boolean) argument
parser.add_argument('--augment', action='store_true', help='Increase output verbosity')
parser.add_argument('--noise', type=float, default=0, help='Scale factor')
parser.add_argument('--epochs', type=int, help='A list of numbers', default=10)

parser.add_argument('--lr', type=float, default=1e-3, help='Scale factor')
parser.add_argument('--momentum', type=float, default=0.9, help='Scale factor')
parser.add_argument('--wd', type=float, default=0.0005, help='Scale factor')

# parser.add_argument('--dataset_a', type=int, nargs='+', help='A list of numbers')
parser.add_argument('--dataset_b', type=int, nargs='+', help='A list of numbers')

# Optional argument with default value
args = parser.parse_args()



class AddGaussianNoise(object):
    def __init__(self, mean=0., std=1.):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        return tensor + torch.randn(tensor.size()) * self.std + self.mean

    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)

def _bn_train_mode(m):
    if isinstance(m, torch.nn.BatchNorm2d):
        m.train()

def hess_vec(vector, loader, model, criterion, cuda=True, bn_train_mode=False):
    param_list = list(model.parameters())
    vector_list = []

    offset = 0
    for param in param_list:
        vector_list.append(vector[offset:offset + param.numel()].detach().view_as(param).to(param.device))
        offset += param.numel()

    model.eval()
    if bn_train_mode:
        model.apply(_bn_train_mode)

    model.zero_grad()
    N = len(loader.dataset)
    for input, target in loader:
        if cuda:
            input = input.cuda(non_blocking=True)
            target = target.cuda(non_blocking=True)
        output = model(input)
        loss = criterion(output, target)
        loss *= input.size()[0] / N

        grad_list = torch.autograd.grad(loss, param_list, create_graph=True)
        dL_dvec = torch.zeros(1, device='cuda' if cuda else 'cpu')
        for v, g in zip(vector_list, grad_list):
            dL_dvec += torch.sum(v * g)
        dL_dvec.backward()

    model.eval()
    return torch.cat([param.grad.view(-1) for param in param_list]).view(-1)



class CurvVecProduct(object):
    def __init__(self, loader, model, criterion, init_vec=None):
        self.loader = loader
        self.model = model
        self.criterion = criterion
        self.init_vec = init_vec
        self.iters = 0
        self.timestamp = time.time()

    def __call__(self, vector):
        if self.iters == 0 and self.init_vec is not None:
            vector = self.init_vec
        start_time = time.time()
        output = hess_vec(
            vector,
            self.loader,
            self.model,
            self.criterion,
            cuda=True,
            bn_train_mode=True
        )
        time_diff = time.time() - start_time
        self.iters += 1
        print('Iter %d. Time: %.2f' % (self.iters, time_diff))
        return output.cpu().unsqueeze(1)



# Now, add the custom noise transformation to your existing pipeline

if args.augment:
    transform_train = transforms.Compose([
        transforms.Resize(32),
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        AddGaussianNoise(0., args.noise),  # Add Gaussian noise with a mean of 0 and std of 0.1
    ])
else:
    transform_train = transforms.Compose([
        transforms.Resize(32),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        AddGaussianNoise(0., args.noise),
    ])

transform_test = transforms.Compose([
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

train_set = torchvision.datasets.CIFAR10(root='./data/', train=True, download=True, transform=transform_train)
test_set = torchvision.datasets.CIFAR10(root='./data/', train=False, download=True, transform=transform_test)

# Function to get a subset of the dataset for a specific class
def get_class_subset(dataset, class_list):
    indices = [i for i, (_, label) in enumerate(dataset) if (label in class_list)]
    subset = Subset(dataset, indices)
    return subset

def create_dataset(class_list):
    dataset_train = get_class_subset(train_set, class_list)  # Class 0 is zerofour
    dataset_test = get_class_subset(test_set, class_list)  # Class 0 is zerofour

    # Create DataLoaders for the zerofour class for both training and testing
    loader_train = DataLoader(dataset_train, batch_size=256, shuffle=True)
    loader_test = DataLoader(dataset_test, batch_size=256, shuffle=False)
    return loader_train, loader_test

a_train, a_test = create_dataset([int(s) for s in args.ckpt.split('/lr')[0].split('/')[-1].split("_")])
b_train, b_test = create_dataset(args.dataset_b)

# model = args.ckpt.split('output/')[1].split('/')[0]
#
# if model == "vgg":
#     print('running VGG model')
#
#     # Define the ResNet-50 model without pre-trained weights
#     model = models.vgg16(pretrained=False)
#     # Modify the last fully connected layer to match the number of classes in CIFAR-10
#     # Modify the last layer of the classifier to match the number of classes in CIFAR-10
#     num_ftrs = model.classifier[6].in_features
#     model.classifier[6] = nn.Linear(num_ftrs, 10)
# elif model == "resnet":
#     print('running ResNet model')
#     model = models.resnet50(pretrained=False)
#     # Modify the last fully connected layer to match the number of classes in CIFAR-10
#     num_ftrs = model.fc.in_features
#     model.fc = nn.Linear(num_ftrs, 10)
#
# model = model.to('cuda')
# model.load_state_dict(torch.load(args.ckpt+'/entire_model.pth'))
#
model = torch.load(args.ckpt+'/entire_model.pth')


# Define a loss function and optimizer
print('using cross entropy loss')
criterion = nn.CrossEntropyLoss()

# Load the checkpoint
checkpoint = torch.load(args.ckpt+'/eigenspace.pth')

# Access the items from the checkpoint
eigvals = checkpoint['eigvals']
gammas = checkpoint['gammas']
W = checkpoint['V']


dataset_numbers = [int(s) for s in args.ckpt.split('/lr')[0].split('/')[-1].split("_")]

# Convert the list of numbers into a space-separated string
dataset_str = " ".join(map(str, dataset_numbers))


# Total number of training steps
total_steps = len(b_train) * 1  # Number of batches * number of epochs
baseline_drop = []
# Define a learning rate scheduler for linear decay
def linear_decay(step):
    return max(0, 1 - step / total_steps)

# Initial learning rate and momentum
lr = args.lr
momentum = args.momentum
weight_decay = args.wd

# Initialize momentum buffers for each parameter
momentum_buffers = {}
for param in model.parameters():
    momentum_buffers[param] = torch.zeros_like(param.data)

# Train the network for 4 epochs using the zerofour_loader_train
for epoch in range(args.epochs):  # Loop over the dataset multiple times if needed

    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data in a_test:
            images, labels = data
            images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    # print('Accuracy of the network on the test zerofour images: %d %%' % (
    #     100 * correct / total))
    print('INITIAL Accuracy of the Loaded Network on the test {} class images: {:.2f} %'.format(dataset_str, (100 * correct / total)))
    model.train()

    running_loss = 0.0
    correct = 0
    total = 0
    for i, data in enumerate(b_train):
        inputs, labels = data
        inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Compute gradients manually
        gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True)

        # Manual SGD with momentum update
        with torch.no_grad():
            for param, grad in zip(model.parameters(), gradients):
                if param in momentum_buffers:
                    momentum_buffers[param] = momentum_buffers[param] * momentum + grad + weight_decay * param
                else:
                    momentum_buffers[param] = grad
                param -= lr * momentum_buffers[param]

        # Update the learning rate
        lr = linear_decay(epoch * len(b_train) + i) * args.lr

        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Print statistics
        running_loss += loss.item()
        if i % 1 == 0:    # Print every 10 mini-batches
            model.eval()

            correct_old = 0
            total_old = 0
            with torch.no_grad():
                for data in a_test:
                    images, labels = data
                    images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
                    outputs = model(images)
                    _, predicted = torch.max(outputs.data, 1)
                    total_old += labels.size(0)
                    correct_old += (predicted == labels).sum().item()

            # print('Accuracy of the network on the test {} images: %d %%'.format(" ".join([str(s) for s in args.dataset_a]) % (
            #     100 * correct_old / total)))
            print(
                'Accuracy on test {} images: {:.2f} %'.format(dataset_str, (100 * correct_old / total_old)))
            baseline_drop.append(100 * correct_old / total)

            model.train()
            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%, LR: {lr:.6f}')

            running_loss = 0.0
            correct = 0
            total = 0




print('Finished Training with flattening out sharp vectors')
print('Trying Training with flattening')
model = torch.load(args.ckpt+'/entire_model.pth')

# Initialize momentum buffers for each parameter
momentum_buffers = {}
for param in model.parameters():
    momentum_buffers[param] = torch.zeros_like(param.data)
method_results = []


# Reshape W to match the shapes of the parameters
W_reshaped = []
offset = 0
for param in model.parameters():
    num_vectors = W.shape[0]  # Number of vectors in W
    for i in range(num_vectors):
        W_reshaped.append(W[i, offset:offset + param.numel()].detach().view_as(param).to(param.device))
    offset += param.numel()

# Train the network for 10 epochs using the fivenine_loader_train
for epoch in range(args.epochs):  # Loop over the dataset multiple times if needed
    running_loss = 0.0
    correct = 0
    total = 0
    for i, data in enumerate(b_train):
        inputs, labels = data
        inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Compute gradients manually
        gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True)

        # Manual SGD with momentum update and dot product removal
        with torch.no_grad():
            for param, grad in zip(model.parameters(), gradients):
                adjusted_grad = grad.clone()
                for w in W_reshaped[offset:offset + param.numel()]:
                    dot_product = torch.sum(grad * w).item()
                    adjusted_grad -= dot_product * w
                offset += param.numel()

                # Update parameters with adjusted gradient
                if param in momentum_buffers:
                    momentum_buffers[param] = momentum_buffers[param] * momentum + adjusted_grad + weight_decay * param
                else:
                    momentum_buffers[param] = adjusted_grad
                param -= lr * momentum_buffers[param]

        # Update the learning rate
        lr = linear_decay(epoch * len(b_train) + i) * args.lr

        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Print statistics
        running_loss += loss.item()
        if i % 1 == 0:    # Print every mini-batch

            model.eval()

            correct_old = 0
            total_old = 0
            with torch.no_grad():
                for data in a_test:
                    images, labels = data
                    images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
                    outputs = model(images)
                    _, predicted = torch.max(outputs.data, 1)
                    total_old += labels.size(0)
                    correct_old += (predicted == labels).sum().item()

            # print('Accuracy of the network on the test zerofour images: %d %%' % (
            #     100 * correct_old / total_old))
            print(
                'Accuracy on test {} images: {:.2f} %'.format(dataset_str, (100 * correct_old / total_old)))
            method_results.append(100 * correct_old / total_old)
            model.train()

            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%, LR: {lr:.6f}')
            running_loss = 0.0
            correct = 0
            total = 0

print('Finished Training')
np.savez(args.ckpt+'/arrays.npz', array1=baseline_drop, array2=method_results)
print('saved arrays under {}'.format(args.ckpt))