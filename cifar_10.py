# -*- coding: utf-8 -*-
"""VGG CIFAR Hessian updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1thL21_zjDfPG3MpCY2kdI0CmKErgWNHH
"""

import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import argparse
# Create a normalized random vector
import time
import gpytorch

parser = argparse.ArgumentParser(description='Example script to demonstrate argparse usage.')
# Positional argument
parser.add_argument('input', type=str, help='Input file path')

# Optional argument
parser.add_argument('--output', type=str, help='Output file path')
parser.add_argument('--model', type=str, help='Output file path', default="vgg")
# Flag (boolean) argument
parser.add_argument('--augment', action='store_true', help='Increase output verbosity')
parser.add_argument('--noise', type=float, default=0, help='Scale factor')
parser.add_argument('--epochs', type=int, help='A list of numbers', default=10)

parser.add_argument('--lr', type=float, default=1e-3, help='Scale factor')
parser.add_argument('--momentum', type=float, default=0.9, help='Scale factor')
parser.add_argument('--wd', type=float, default=0.0005, help='Scale factor')

parser.add_argument('--dataset_a', type=int, nargs='+', help='A list of numbers')
parser.add_argument('--dataset_b', type=int, nargs='+', help='A list of numbers')

# Optional argument with default value
parser.add_argument('--scale', type=float, default=1.0, help='Scale factor')
args = parser.parse_args()

import torch
from torchvision import transforms
from PIL import Image
import numpy as np


class AddGaussianNoise(object):
    def __init__(self, mean=0., std=1.):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        return tensor + torch.randn(tensor.size()) * self.std + self.mean

    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)

def _bn_train_mode(m):
    if isinstance(m, torch.nn.BatchNorm2d):
        m.train()

def hess_vec(vector, loader, model, criterion, cuda=True, bn_train_mode=False):
    param_list = list(model.parameters())
    vector_list = []

    offset = 0
    for param in param_list:
        vector_list.append(vector[offset:offset + param.numel()].detach().view_as(param).to(param.device))
        offset += param.numel()

    model.eval()
    if bn_train_mode:
        model.apply(_bn_train_mode)

    model.zero_grad()
    N = len(loader.dataset)
    for input, target in loader:
        if cuda:
            input = input.cuda(non_blocking=True)
            target = target.cuda(non_blocking=True)
        output = model(input)
        loss = criterion(output, target)
        loss *= input.size()[0] / N

        grad_list = torch.autograd.grad(loss, param_list, create_graph=True)
        dL_dvec = torch.zeros(1, device='cuda' if cuda else 'cpu')
        for v, g in zip(vector_list, grad_list):
            dL_dvec += torch.sum(v * g)
        dL_dvec.backward()

    model.eval()
    return torch.cat([param.grad.view(-1) for param in param_list]).view(-1)



class CurvVecProduct(object):
    def __init__(self, loader, model, criterion, init_vec=None):
        self.loader = loader
        self.model = model
        self.criterion = criterion
        self.init_vec = init_vec
        self.iters = 0
        self.timestamp = time.time()

    def __call__(self, vector):
        if self.iters == 0 and self.init_vec is not None:
            vector = self.init_vec
        start_time = time.time()
        output = hess_vec(
            vector,
            self.loader,
            self.model,
            self.criterion,
            cuda=True,
            bn_train_mode=True
        )
        time_diff = time.time() - start_time
        self.iters += 1
        print('Iter %d. Time: %.2f' % (self.iters, time_diff))
        return output.cpu().unsqueeze(1)



# Now, add the custom noise transformation to your existing pipeline

if args.augment:
    transform_train = transforms.Compose([
        transforms.Resize(32),
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        AddGaussianNoise(0., args.noise),  # Add Gaussian noise with a mean of 0 and std of 0.1
    ])
else:
    transform_test = transforms.Compose([
        transforms.Resize(32),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        AddGaussianNoise(0., args.noise),
    ])

transform_test = transforms.Compose([
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

train_set = torchvision.datasets.CIFAR10(root='./data/', train=True, download=True, transform=transform_train)
test_set = torchvision.datasets.CIFAR10(root='./data/', train=False, download=True, transform=transform_test)

# Function to get a subset of the dataset for a specific class
def get_class_subset(dataset, class_list):
    indices = [i for i, (_, label) in enumerate(dataset) if (label in class_list)]
    subset = Subset(dataset, indices)
    return subset

def create_dataset(class_list)
    dataset_train = get_class_subset(train_set, class_list)  # Class 0 is zerofour
    dataset_test = get_class_subset(test_set, class_list)  # Class 0 is zerofour

    # Create DataLoaders for the zerofour class for both training and testing
    loader_train = DataLoader(dataset_train, batch_size=256, shuffle=True)
    loader_test = DataLoader(dataset_test, batch_size=256, shuffle=False)
    return loader_train, loader_test

a_train, a_test = create_dataset(args.dataset_a)
b_train, b_test = create_dataset(args.dataset_b)

if args.model == "vgg":


    # Define the ResNet-50 model without pre-trained weights
    model = models.vgg16(pretrained=False)
    # Modify the last fully connected layer to match the number of classes in CIFAR-10
    # Modify the last layer of the classifier to match the number of classes in CIFAR-10
    num_ftrs = model.classifier[6].in_features
    model.classifier[6] = nn.Linear(num_ftrs, 10)
elif args.model == "resnet":
    model = models.resnet50(pretrained=False)
    # Modify the last fully connected layer to match the number of classes in CIFAR-10
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, 10)

model = model.to('cuda')

# Define a loss function and optimizer
criterion = nn.CrossEntropyLoss()
# Assuming zerofour_loader_train is defined elsewhere and is your DataLoader
total_steps = len(a_train) * args.epochs  # Number of batches * number of epochs

# Define a learning rate scheduler for linear decay
def linear_decay(step):
    return max(0, 1 - step / total_steps)


# Initial learning rate and momentum
lr = args.lr
momentum = args.momentum
weight_decay = args.wd

# Initialize momentum buffers for each parameter
momentum_buffers = {}
for param in model.parameters():
    momentum_buffers[param] = torch.zeros_like(param.data)

# Train the network for 10 epochs using the zerofour_loader_train
for epoch in range(args.epochs):  # Loop over the dataset multiple times if needed
    running_loss = 0.0
    correct = 0
    total = 0
    for i, data in enumerate(a_train):
        inputs, labels = data
        inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Compute gradients manually
        gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True)

        # Manual SGD with momentum update
        with torch.no_grad():
            for param, grad in zip(model.parameters(), gradients):
                if param in momentum_buffers:
                    momentum_buffers[param] = momentum_buffers[param] * momentum + grad + weight_decay * param
                else:
                    momentum_buffers[param] = grad
                param -= lr * momentum_buffers[param]

        # Update the learning rate
        lr = linear_decay(epoch * len(a_train) + i) * args.lr

        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Print statistics
        running_loss += loss.item()
        if i % 10 == 9:    # Print every 10 mini-batches
            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%, LR: {lr:.6f}')
            running_loss = 0.0
            correct = 0
            total = 0

print('Finished Training')

# Set the model to evaluation mode
model.eval()

correct = 0
total = 0
with torch.no_grad():
    for data in a_train:
        images, labels = data
        images, labels = images.to('cuda'), labels.to('cuda')
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the test {} images: %d %%'.format(" ".join(args.dataset_a)) % (
    100 * correct / total))
model.train()

# Set the model to evaluation mode
model.eval()

correct = 0
total = 0
total_loss = 0.0  # Initialize total loss

with torch.no_grad():
    for data in b_test:
        images, labels = data
        images, labels = images.to('cuda'), labels.to('cuda')
        outputs = model(images)

        loss = criterion(outputs, labels)  # Compute the loss
        total_loss += loss.item() * images.size(0)  # Update total loss

        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

# Calculate average loss
average_loss = total_loss / total

print(f'Accuracy of the network on the test {" ".join(args.dataset_b)} images: {100 * correct / total:.2f}%')
print(f'Average loss on the test {" ".join(args.dataset_b)} images: {average_loss:.4f}')

print('running Lanczos on original training set model')

P = sum(p.numel() for p in model.parameters())
random_vec = torch.randn(P, device='cpu')
random_vec = random_vec / torch.norm(random_vec)

# Pass the random vector as the initial vector to the CurvVecProduct
productor = CurvVecProduct(a_train, model, criterion, init_vec=random_vec)

# Run the Lanczos algorithm
lanczos_iters = 30
Q, T = gpytorch.utils.lanczos.lanczos_tridiag(
    productor,
    max_iter=lanczos_iters,
    dtype=torch.float32,
    device='cpu',
    matrix_shape=(P, P)
)

eigvals, eigvects = torch.linalg.eigh(T)
eigvals = eigvals

gammas = eigvects[0, :] ** 2
V = eigvects.t() @ Q.t()

# Save or return the results as needed
result = {
    # 'w': w,
    'eigvals': eigvals,
    'gammas': gammas,
    'V': V
}
print(eigvals)

"""save eigenvalues"""
# Set the model to evaluation mode
vgg16.eval()

correct = 0
total = 0
with torch.no_grad():
    for data in zerofour_loader_test:
        images, labels = data
        images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
        outputs = resnet50(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the test zerofour images: %d %%' % (
    100 * correct / total))

W = V

print('here we run vanilla SGD with no Eigenvector saving')
vgg16.train()

# Define a loss function
criterion = nn.CrossEntropyLoss()

# Total number of training steps
total_steps = len(zerofour_loader_train) * 1  # Number of batches * number of epochs

# Define a learning rate scheduler for linear decay
def linear_decay(step):
    return max(0, 1 - step / total_steps)

# Initial learning rate and momentum
lr = 0.01
momentum = 0.9
weight_decay = 5e-4

# Initialize momentum buffers for each parameter
momentum_buffers = {}
for param in resnet50.parameters():
    momentum_buffers[param] = torch.zeros_like(param.data)

# Train the network for 4 epochs using the zerofour_loader_train
for epoch in range(1):  # Loop over the dataset multiple times if needed

    resnet50.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data in zerofour_loader_test:
            images, labels = data
            images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
            outputs = resnet50(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the test zerofour images: %d %%' % (
        100 * correct / total))
    vgg16.train()

    running_loss = 0.0
    correct = 0
    total = 0
    for i, data in enumerate(fivenine_loader_train):
        inputs, labels = data
        inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU

        # Forward pass
        outputs = resnet50(inputs)
        loss = criterion(outputs, labels)

        # Compute gradients manually
        gradients = torch.autograd.grad(loss, resnet50.parameters(), create_graph=True)

        # Manual SGD with momentum update
        with torch.no_grad():
            for param, grad in zip(resnet50.parameters(), gradients):
                if param in momentum_buffers:
                    momentum_buffers[param] = momentum_buffers[param] * momentum + grad + weight_decay * param
                else:
                    momentum_buffers[param] = grad
                param -= lr * momentum_buffers[param]

        # Update the learning rate
        lr = linear_decay(epoch * len(zerofour_loader_train) + i) * 0.01

        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Print statistics
        running_loss += loss.item()
        if i % 1 == 0:    # Print every 10 mini-batches
            vgg16.eval()

            correct_old = 0
            total_old = 0
            with torch.no_grad():
                for data in zerofour_loader_test:
                    images, labels = data
                    images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
                    outputs = vgg16(images)
                    _, predicted = torch.max(outputs.data, 1)
                    total_old += labels.size(0)
                    correct_old += (predicted == labels).sum().item()

            print('Accuracy of the network on the test zerofour images: %d %%' % (
                100 * correct_old / total))
            vgg16.train()
            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%, LR: {lr:.6f}')

            running_loss = 0.0
            correct = 0
            total = 0




print('Finished Training')

import torch
import torch.nn as nn
from torchvision import models

resnet50.eval()

correct = 0
total = 0
with torch.no_grad():
    for data in zerofour_loader_test:
        images, labels = data
        images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
        outputs = resnet50(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print('Accuracy of the network on the test zerofour images: %d %%' % (
    100 * correct / total))
resnet50.train()

# Define a loss function
criterion = nn.CrossEntropyLoss()

# Total number of training steps
total_steps = len(fivenine_loader_train) * 10  # Number of batches * number of epochs

# Define a learning rate scheduler for linear decay
def linear_decay(step):
    return max(0, 1 - step / total_steps)

# Initial learning rate and momentum
lr = 0.01
momentum = 0.9
weight_decay = 5e-4

# Initialize momentum buffers for each parameter
momentum_buffers = {}
for param in resnet50.parameters():
    momentum_buffers[param] = torch.zeros_like(param.data)

# Reshape W to match the shapes of the parameters
W_reshaped = []
offset = 0
for param in resnet50.parameters():
    num_vectors = W.shape[0]  # Number of vectors in W
    for i in range(num_vectors):
        W_reshaped.append(W[i, offset:offset + param.numel()].detach().view_as(param).to(param.device))
    offset += param.numel()

# Train the network for 10 epochs using the fivenine_loader_train
for epoch in range(10):  # Loop over the dataset multiple times if needed
    running_loss = 0.0
    correct = 0
    total = 0
    for i, data in enumerate(fivenine_loader_train):
        inputs, labels = data
        inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU

        # Forward pass
        outputs = resnet50(inputs)
        loss = criterion(outputs, labels)

        # Compute gradients manually
        gradients = torch.autograd.grad(loss, resnet50.parameters(), create_graph=True)

        # Manual SGD with momentum update and dot product removal
        with torch.no_grad():
            for param, grad in zip(resnet50.parameters(), gradients):
                adjusted_grad = grad.clone()
                for w in W_reshaped[offset:offset + param.numel()]:
                    dot_product = torch.sum(grad * w).item()
                    adjusted_grad -= dot_product * w
                offset += param.numel()

                # Update parameters with adjusted gradient
                if param in momentum_buffers:
                    momentum_buffers[param] = momentum_buffers[param] * momentum + adjusted_grad + weight_decay * param
                else:
                    momentum_buffers[param] = adjusted_grad
                param -= lr * momentum_buffers[param]

        # Update the learning rate
        lr = linear_decay(epoch * len(fivenine_loader_train) + i) * 0.01

        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Print statistics
        running_loss += loss.item()
        if i % 1 == 0:    # Print every mini-batch

            resnet50.eval()

            correct_old = 0
            total_old = 0
            with torch.no_grad():
                for data in zerofour_loader_test:
                    images, labels = data
                    images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
                    outputs = resnet50(images)
                    _, predicted = torch.max(outputs.data, 1)
                    total_old += labels.size(0)
                    correct_old += (predicted == labels).sum().item()

            print('Accuracy of the network on the test zerofour images: %d %%' % (
                100 * correct_old / total_old))
            resnet50.train()

            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%, LR: {lr:.6f}')
            running_loss = 0.0
            correct = 0
            total = 0

print('Finished Training')

# Set the model to evaluation mode
resnet50.eval()

correct = 0
total = 0
with torch.no_grad():
    for data in fivenine_loader_test:
        images, labels = data
        images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
        outputs = resnet50(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the test fivenine images: %d %%' % (
    100 * correct / total))

# Set the model to evaluation mode
resnet50.eval()

correct = 0
total = 0
with torch.no_grad():
    for data in zerofour_loader_test:
        images, labels = data
        images, labels = images.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU
        outputs = resnet50(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the test zerofour images: %d %%' % (
    100 * correct / total))

import torch
import torchvision
import matplotlib.pyplot as plt
import numpy as np
import random

# Function to show an image
def imshow(img):
    img = img / 2 + 0.5     # Unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Randomly sample 20 images from the zerofour_loader_test
random_indices = random.sample(range(len(zerofour_loader_test.dataset)), 20)
sampled_images = torch.stack([zerofour_loader_test.dataset[i][0] for i in random_indices])
sampled_labels = torch.tensor([zerofour_loader_test.dataset[i][1] for i in random_indices])

# Show the images
imshow(torchvision.utils.make_grid(sampled_images, nrow=5))

# Print the true labels
print('True labels: ', ' '.join(f'{sampled_labels[j]}' for j in range(len(sampled_labels))))
sampled_images = sampled_images.to('cuda')
# Print the predicted labels
outputs = resnet50(sampled_images)
_, predicted = torch.max(outputs, 1)
print('Pred labels: ', ' '.join(f'{predicted[j]}' for j in range(len(predicted))))

import matplotlib.pyplot as plt
import numpy as np

# Function to show an image
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Get some random training images
dataiter = iter(fivenine_loader_train)
images, labels = next(dataiter)

# Show images
imshow(torchvision.utils.make_grid(images[:4]))

resnet50.train()

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models

# Define the ResNet-50 model without pre-trained weights
resnet50 = models.resnet50(pretrained=False)
# Modify the last fully connected layer to match the number of classes in CIFAR-10
num_ftrs = resnet50.fc.in_features
resnet50.fc = nn.Linear(num_ftrs, 10)
resnet50 = resnet50.to('cuda')

# Define a loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(resnet50.parameters(), lr=0.02, momentum=0.9, weight_decay=5e-4)

# Total number of training steps
total_steps = len(fivenine_loader_train) * 4  # Number of batches * number of epochs

# Define a learning rate scheduler for linear decay
def linear_decay(step):
    return max(0, 1 - step / total_steps)

scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=linear_decay)

# Train the network for 4 epochs using the zerofour_loader_train
for epoch in range(1):  # Loop over the dataset multiple times if needed
    running_loss = 0.0
    for i, data in enumerate(fivenine_loader_train):
        inputs, labels = data
        random_noise = normalize(torch.randn(inputs.shape))

        # Generate a vector of uniformly sampled integers from 0 to k-1
        random_labels = torch.randint(low=0, high=10, size=(labels.shape[0],))
        inputs = torch.cat((inputs,random_noise),dim=0)
        labels = torch.cat((labels,random_labels),dim=0)

        inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move inputs and labels to GPU

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + backward + optimize
        outputs = resnet50(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Update the learning rate
        scheduler.step()

        # Print statistics
        running_loss += loss.item()
        if i % 10 == 9:    # Print every 10 mini-batches
            print(f'Input size: {inputs.shape}')
            print(f'Labels: {sum(labels[:256])/256}')
            print(f'Labels: {sum(labels[256:])/256}')
            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 10}, LR: {scheduler.get_last_lr()[0]}')
            running_loss = 0.0

print('Finished Training')

for i, data in enumerate(fivenine_loader_train):
    inputs, labels = data
    random_noise = torch.randn(inputs.shape)
    # Generate a vector of uniformly sampled integers from 0 to k-1
    random_labels = torch.randint(low=0, high=10, size=(labels.shape[0],))
    inputs = torch.cat((inputs,random_noise),dim=0)
    labels = torch.cat((labels,random_labels),dim=0)
    break

labels

import torch
import torchvision
import matplotlib.pyplot as plt
import numpy as np
import random

# Function to show an image
def imshow(img):
    img = img / 2 + 0.5     # Unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Randomly sample 20 images from the zerofour_loader_test
random_indices = random.sample(range(len(zerofour_loader_test.dataset)), 20)
sampled_images = torch.stack([zerofour_loader_test.dataset[i][0] for i in random_indices])
sampled_labels = torch.tensor([zerofour_loader_test.dataset[i][1] for i in random_indices])

# Show the images
imshow(torchvision.utils.make_grid(sampled_images, nrow=5))

# Print the true labels
print('True labels: ', ' '.join(f'{sampled_labels[j]}' for j in range(len(sampled_labels))))

# Print the predicted labels
sampled_images = sampled_images.to('cuda')
outputs = resnet50(sampled_images)
_, predicted = torch.max(outputs, 1)
print('Predicted labels:', ' '.join(f'{predicted[j]}' for j in range(len(predicted))))

# Set the model to evaluation mode
resnet50.eval()

correct = 0
total = 0
with torch.no_grad():
    for data in zerofour_loader_test:
        images, labels = data
        images, labels = images.to('cuda'), labels.to('cuda')
        outputs = resnet50(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the test zerofour images: %d %%' % (
    100 * correct / total))

# Set the model to evaluation mode
resnet50.eval()

correct = 0
total = 0
with torch.no_grad():
    for data in fivenine_loader_test:
        images, labels = data
        images, labels = images.to('cuda'), labels.to('cuda')
        outputs = resnet50(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the test fivenine images: %d %%' % (
    100 * correct / total))

import matplotlib.pyplot as plt
import numpy as np

# Function to show an image
def imshow(img):
    img = img / 2 + 0.5     # Unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# Get a batch of test images and labels
dataiter = iter(one_loader_test)
images, labels = next(dataiter)

# Show the images
imshow(torchvision.utils.make_grid(images))

# Print the true labels
print('True labels: ', ' '.join(f'{labels[j]}' for j in range(len(labels))))
images = images.to('cuda')
# Print the predicted labels
outputs = resnet50(images)
_, predicted = torch.max(outputs, 1)
print('Predicted labels:', ' '.join(f'{predicted[j]}' for j in range(len(predicted))))

